{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCK7tbbZAy6f93Kd6nArf4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagittariusk2/fake-news-detection/blob/main/RR_version_of_fake_news_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q transformers"
      ],
      "metadata": {
        "id": "pHQp0MDgofNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5705c80-2a4a-4d09-83ec-a6704651b3be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUD1i3X5B1yH",
        "outputId": "2a56c0e4-2bb6-4726-c305-40f508657124"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import urllib.parse\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "metadata": {
        "id": "nhYBD6P_iBry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20dfa71-e2af-44e9-b206-761b625d6515"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9223372036854775807"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Feature Extraction\n",
        "    ## Complexity Features\n",
        "        # 01. special_character <-\n",
        "        # 02. upper_case <-\n",
        "        # 03. lower_case <-\n",
        "        # 04. short_sentences <-\n",
        "        # 05. long_sentences <-\n",
        "        # 06. question_marks <-\n",
        "        # 07. exclamation_marks <-\n",
        "    ## Stylometric Features\n",
        "        # 08. adjective_count <-\n",
        "        # 09. adverb_count <-\n",
        "    ## \n",
        "        # 10. hyperlinks <-\n",
        "        # 11. hyperlinks_diversity <-\n",
        "        # 12. phising_score <-\n",
        "        # 13. sensational_score <-\n",
        "        # 14. sponsorship_score <-\n",
        "        # 15. headline_content_symmetry_score <-"
      ],
      "metadata": {
        "id": "JMn-uGPo8fiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 01. special_character\n",
        "\n",
        "specialCharecters = \"`~!@#$%^&*()_-+={}[]:;'<>,.?/\\\\|\\\"\"\n",
        "\n",
        "def special_character_fe(content, headline=\"\"):\n",
        "  ans = 0\n",
        "  for i in headline:\n",
        "    if i in specialCharecters:\n",
        "      ans += 1\n",
        "  for i in content:\n",
        "    if i in specialCharecters:\n",
        "      ans += 1\n",
        "  return ans"
      ],
      "metadata": {
        "id": "i36FMUKu30RA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 02. upper_case\n",
        "def upper_case_fe(content, headline=\"\"):\n",
        "  ans = 0\n",
        "  for i in headline:\n",
        "    if i<='Z' and i>='A':\n",
        "      ans += 1\n",
        "  for i in content:\n",
        "    if i<='Z' and i>='A':\n",
        "      ans += 1\n",
        "  return ans"
      ],
      "metadata": {
        "id": "hm_uPfjH30AU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 03. lower_case\n",
        "def lower_case_fe(content, headline=\"\"):\n",
        "  ans = 0\n",
        "  for i in headline:\n",
        "    if i<='z' and i>='a':\n",
        "      ans += 1\n",
        "  for i in content:\n",
        "    if i<='z' and i>='a':\n",
        "      ans += 1\n",
        "  return ans"
      ],
      "metadata": {
        "id": "tWPIA3zF3x7C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 04. short_sentences\n",
        "# 05. long_sentences\n",
        "\n",
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "digits = \"([0-9])\"\n",
        "multiple_dots = r'\\.{2,}'\n",
        "\n",
        "def split_into_sentences(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text into sentences.\n",
        "\n",
        "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead \n",
        "    to incorrect splitting because they are used as markers for splitting.\n",
        "\n",
        "    :param text: text to be split into sentences\n",
        "    :type text: str\n",
        "\n",
        "    :return: list of sentences\n",
        "    :rtype: list[str]\n",
        "    \"\"\"\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
        "    return sentences\n",
        "\n",
        "def length_sentences_fe(content, headline=\"\"):\n",
        "  headline_sentences = split_into_sentences(headline)\n",
        "  content_sentences = split_into_sentences(content)\n",
        "  short_sentence_cnt = 0\n",
        "  long_sentence_cnt = 0\n",
        "  for i in headline_sentences:\n",
        "    if len(i)<=15:\n",
        "      short_sentence_cnt += 1\n",
        "    else:\n",
        "      long_sentence_cnt += 1\n",
        "  for i in content_sentences:\n",
        "    if len(i)<=15:\n",
        "      short_sentence_cnt += 1\n",
        "    else:\n",
        "      long_sentence_cnt += 1\n",
        "  return [short_sentence_cnt, long_sentence_cnt]"
      ],
      "metadata": {
        "id": "7Rv4yA5W3xlP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VQzOCEmoV-Ol"
      },
      "outputs": [],
      "source": [
        "# 06. Question Marks\n",
        "\n",
        "def question_marks_fe(content, headline=\"\"):\n",
        "  ans = 0\n",
        "  for i in headline:\n",
        "    if i=='?':\n",
        "      ans += 1\n",
        "  for i in content:\n",
        "    if i=='?':\n",
        "      ans += 1\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 07. Exclamation Marks\n",
        "\n",
        "def exclamation_marks_fe(content, headline=\"\"):\n",
        "  ans = 0\n",
        "  for i in headline:\n",
        "    if i=='!':\n",
        "      ans += 1\n",
        "  for i in content:\n",
        "    if i=='!':\n",
        "      ans += 1\n",
        "  return ans"
      ],
      "metadata": {
        "id": "cjjTjhMYhNkI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 08. adjective_count\n",
        "\n",
        "def adjective_count_fe(content, headline=\"\"):\n",
        "  JJ_count = 0\n",
        "  content_tokenized = sent_tokenize(content)\n",
        "  for i in content_tokenized:\n",
        "    content_token = word_tokenize(i)\n",
        "    content_token = [w for w in content_token if not w in stop_words]\n",
        "    content_tagged = pos_tag(content_token)\n",
        "    for i in content_tagged:\n",
        "      if i[1]=='JJ':\n",
        "        JJ_count += 1\n",
        "\n",
        "  headline_tokenized = sent_tokenize(headline)\n",
        "  for i in headline_tokenized:\n",
        "    headline_token = word_tokenize(i)\n",
        "    headline_token = [w for w in headline_token if not w in stop_words]\n",
        "    headline_tagged = pos_tag(headline_token)\n",
        "    for i in headline_tokenized:\n",
        "      if i[1]=='JJ':\n",
        "        JJ_count += 1\n",
        "\n",
        "  return JJ_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEPaN_NREoTp",
        "outputId": "09a6784b-96d6-4af6-dc7c-d9bd60a97c17"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 09. adverb_count\n",
        "\n",
        "def adverb_count_fe(content, headline=\"\"):\n",
        "  RB_count = 0\n",
        "  content_tokenized = sent_tokenize(content)\n",
        "  for i in content_tokenized:\n",
        "    content_token = word_tokenize(i)\n",
        "    content_token = [w for w in content_token if not w in stop_words]\n",
        "    content_tagged = pos_tag(content_token)\n",
        "    for i in content_tagged:\n",
        "      if i[1]=='RB':\n",
        "        RB_count += 1\n",
        "\n",
        "  headline_tokenized = sent_tokenize(headline)\n",
        "  for i in headline_tokenized:\n",
        "    headline_token = word_tokenize(i)\n",
        "    headline_token = [w for w in headline_token if not w in stop_words]\n",
        "    headline_tagged = pos_tag(headline_token)\n",
        "    for i in headline_tokenized:\n",
        "      if i[1]=='RB':\n",
        "        RB_count += 1\n",
        "\n",
        "  return RB_count"
      ],
      "metadata": {
        "id": "Jmm66KhlEmv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Hyperlinks\n",
        "# 11. Hyperlinks Diversity\n",
        "# 12. Phishing URL Score\n",
        "\n",
        "phising_url_mpp = dict()\n",
        "\n",
        "def phishing_url_score_fe(phising_url):\n",
        "  API_key ='6874343ff58ee2ea10dae6ec40978e9647b03a1dbb72870295bcb540fd41738f'\n",
        "  url = 'https://www.virustotal.com/vtapi/v2/url/report'\n",
        "  res = []\n",
        "  if phising_url in phising_url_mpp.keys():\n",
        "    if phising_url_mpp[phising_url]!=900:\n",
        "      return phising_url_mpp[phising_url]\n",
        "  parameters = {'apikey': API_key, 'resource': url}\n",
        "  try:\n",
        "    time.sleep(16)\n",
        "    response= requests.get(url=url, params=parameters,timeout=5)\n",
        "    if response.status_code==200:\n",
        "      json_response= json.loads(response.text)\n",
        "      if json_response['response_code']==1:\n",
        "        phising_url_mpp[phising_url]=json_response['positives']\n",
        "      else:\n",
        "        phising_url_mpp[phising_url]=90\n",
        "    else:\n",
        "      phising_url_mpp[phising_url]=90\n",
        "  except:\n",
        "    phising_url_mpp[phising_url]=900\n",
        "  return phising_url_mpp[phising_url]\n",
        "\n",
        "def hyperLinks_fe(content, headline=\"\"):\n",
        "  headline_urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', headline)\n",
        "  content_urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', content)\n",
        "  \n",
        "  mpp = dict()\n",
        "\n",
        "  for i in headline_urls:\n",
        "    domain = urllib.parse.urlparse(i).netloc\n",
        "    if domain in mpp.keys():\n",
        "      mpp[domain] += 1\n",
        "    else:\n",
        "      mpp[domain] = 1\n",
        "  \n",
        "  for i in content_urls:\n",
        "    domain = urllib.parse.urlparse(i).netloc\n",
        "    if domain in mpp.keys():\n",
        "      mpp[domain] += 1\n",
        "    else:\n",
        "      mpp[domain] = 1\n",
        "  cnt = 0\n",
        "  ans = 0\n",
        "  phising_score = 0\n",
        "  for i in mpp:\n",
        "    cnt += mpp[i]\n",
        "    ans += (mpp[i] * mpp[i])\n",
        "    phising_score += phishing_url_score_fe(i)*mpp[i]\n",
        "  return [cnt, ans, phising_score]\n"
      ],
      "metadata": {
        "id": "Dlz7IEr-hcPl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Sensational Score\n",
        "\n",
        "def sentimental_score_fe(content, headline=\"\"):\n",
        "  sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "  return (sentiment_pipeline(headline+\". \"+content)[0]['score'])"
      ],
      "metadata": {
        "id": "Z-6u1WJpXD6x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. sponsorship_score\n",
        "\n",
        "sponsorship_words = ['adherent', 'advocacy', 'advocate', 'aegis', 'aid', 'alms', 'angel', 'atonement', 'auspices', 'backer', 'backing', 'bene-faction', 'benefactor', 'beneficence', 'bid', 'charity', 'contribution', 'egis', 'endeavor', 'essay', 'expiation', 'feeler', 'gift', 'godparent', 'grubstaker', 'guarantor', 'guardianship', 'hit', 'mainstay', 'oblation', 'offer', 'offered', 'offering', 'overture', 'pass', 'patron', 'patron', 'patronage', 'pitch', 'present', 'presentation', 'promoter', 'proposition', 'propoundment', 'protectorship', 'rendition', 'sacrifice', 'sponsor', 'submission', 'subscription', 'support', 'supporter', 'surety', 'sustainer', 'tender', 'tutelage', 'underwriter']\n",
        "def sponsorship_score_fe(content, headline=\"\"):\n",
        "  headline_token = word_tokenize(headline)\n",
        "  content_token = word_tokenize(content)\n",
        "  text_stem = []\n",
        "  ans = 0\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for i in headline_token:\n",
        "    text_stem.append(lemmatizer.lemmatize(i))\n",
        "  for i in content_token:\n",
        "    text_stem.append(lemmatizer.lemmatize(i))\n",
        "  for i in text_stem:\n",
        "    if i in sponsorship_words:\n",
        "      ans += 1\n",
        "  return ans"
      ],
      "metadata": {
        "id": "_KHR1Jxaz4dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. headline_content_symmetry_score\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # Tokenize text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "\n",
        "  # Return preprocessed text\n",
        "  return ' '.join(lemmatized_tokens)\n",
        "\n",
        "def headline_content_symmetry_score_fe(content, headline=\"\"):\n",
        "  if headline==\"\":\n",
        "    return 1.0\n",
        "\n",
        "  preprocessed_text1 = preprocess_text(content)\n",
        "  preprocessed_text2 = preprocess_text(headline)\n",
        "\n",
        "  vectorizer = CountVectorizer().fit_transform([preprocessed_text1, preprocessed_text2])\n",
        "  similarity = cosine_similarity(vectorizer)[0][1]\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "5z9dnCD6EkiV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_feature(news):\n",
        "  news['question_marks'] = question_marks_fe(news['headline'], news['content'])\n",
        "  news['exclamation_marks'] = exclamation_marks_fe(news['headline'], news['content'])\n",
        "  hyperLinks = hyperLinks_fe(news['headline'], news['content'])\n",
        "  news['hyperlinks'] = hyperLinks[0]\n",
        "  news['hyperlinks_diversity'] = hyperLinks[1]\n",
        "  news['phising_score'] = hyperLinks[2]\n",
        "  # news['sensational_score'] = sentimental_fe(news['headline'], news['content'])\n",
        "  return news"
      ],
      "metadata": {
        "id": "Kfa9VE3Lje2Z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('phising_url.csv', newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  for i in reader:\n",
        "    phising_url_mpp[i['url']]=int(i['score'])"
      ],
      "metadata": {
        "id": "RSGr9LMjXLjR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open files and extract feature\n",
        "\n",
        "trained_dataset = []\n",
        "id = 0\n",
        "\n",
        "with open('news.csv', newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  for i in reader:\n",
        "    label = 0\n",
        "    if i['label']=='REAL':\n",
        "      label = 1\n",
        "    news = {'id':id, 'headline':i['title'], 'content':i['text'], 'label':label}\n",
        "    news = add_feature(news)\n",
        "    id += 1\n",
        "    trained_dataset.append(news)\n",
        "\n",
        "with open('train.csv', newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  for i in reader:\n",
        "    news = {'id':id, 'headline':i['title'], 'content':i['text'], 'label':i['label']}\n",
        "    news = add_feature(news)\n",
        "    id += 1\n",
        "    trained_dataset.append(news)\n",
        "\n",
        "with open('True.csv', newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  for i in reader:\n",
        "    news = {'id':id, 'headline':i['title'], 'content':i['text'], 'label':1}\n",
        "    news = add_feature(news)\n",
        "    id += 1\n",
        "    trained_dataset.append(news)\n",
        "\n",
        "# print(trained_dataset)\n",
        "\n",
        "# train the model"
      ],
      "metadata": {
        "id": "QFB8fNLPLRAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}